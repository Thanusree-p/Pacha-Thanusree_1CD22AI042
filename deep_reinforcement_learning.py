# -*- coding: utf-8 -*-
"""Deep_Reinforcement_Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FzPVZBUWWBjFXH7BSSjZvBmGPebA_sd5
"""

# -- coding: utf-8 --
"""
Deep Reinforcement Learning - Q-Learning Example
Improved version with epsilon-greedy, learning rate, and modular design
"""

import numpy as np
import pylab as pl
import networkx as nx

# ---------------------------
# GRAPH SETUP
# ---------------------------
edges = [(0, 1), (1, 5), (5, 6), (5, 4), (1, 2),
         (1, 3), (9, 10), (2, 4), (0, 6), (6, 7),
         (8, 9), (7, 8), (1, 7), (3, 9)]

goal = 10
MATRIX_SIZE = 11
gamma = 0.8     # discount factor
alpha = 0.7     # learning rate
epsilon = 0.3   # exploration probability

# Create reward matrix
M = np.matrix(np.ones((MATRIX_SIZE, MATRIX_SIZE)) * -1)

for (x, y) in edges:
    M[x, y] = 100 if y == goal else 0
    M[y, x] = 100 if x == goal else 0

M[goal, goal] = 100

# Draw initial graph
G = nx.Graph()
G.add_edges_from(edges)
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=600, font_size=10)
pl.title("Environment Graph")
pl.show()

# ---------------------------
# Q-LEARNING SETUP
# ---------------------------
Q = np.zeros((MATRIX_SIZE, MATRIX_SIZE))

def available_actions(state):
    """Return possible actions from the given state."""
    return np.where(M[state] >= 0)[1]

def choose_action(state):
    """Choose action using epsilon-greedy policy."""
    if np.random.uniform(0, 1) < epsilon:
        # Explore
        return int(np.random.choice(available_actions(state)))
    else:
        # Exploit
        return int(np.argmax(Q[state]))

def update_Q(current_state, action):
    """Perform the Q-learning update."""
    next_state = action
    reward = M[current_state, action]
    max_future_Q = np.max(Q[next_state])

    Q[current_state, action] = Q[current_state, action] + alpha * (
        reward + gamma * max_future_Q - Q[current_state, action]
    )

# ---------------------------
# TRAINING PHASE
# ---------------------------
episodes = 1000
rewards = []

for i in range(episodes):
    state = np.random.randint(0, MATRIX_SIZE)
    total_reward = 0

    while state != goal:
        action = choose_action(state)
        update_Q(state, action)
        total_reward += M[state, action]
        state = action

    rewards.append(total_reward)

# ---------------------------
# RESULTS
# ---------------------------
print("Trained Q matrix:")
print(np.round(Q / np.max(Q) * 100, 2))

# Plot training progress
pl.plot(rewards)
pl.xlabel("Episode")
pl.ylabel("Reward per Episode")
pl.title("Training Progress")
pl.show()

# ---------------------------
# TESTING - FIND OPTIMAL PATH
# ---------------------------
state = 0
path = [state]

while state != goal:
    next_state = np.argmax(Q[state])
    path.append(next_state)
    state = next_state

print("Most efficient path found:", path)

# ---------------------------
# ENVIRONMENT FEEDBACK (Police, Drugs)
# ---------------------------
police = [2, 4, 5]
drug_traces = [3, 8, 9]

G = nx.Graph()
G.add_edges_from(edges)
mapping = {0:'Detective(0)', 1:'1', 2:'Police(2)', 3:'Drugs(3)',
           4:'Police(4)', 5:'Police(5)', 6:'6', 7:'7',
           8:'Drugs(8)', 9:'Drugs(9)', 10:'Racket(10)'}

H = nx.relabel_nodes(G, mapping)
pos = nx.spring_layout(H)
nx.draw(H, pos, with_labels=True, node_color='lightgreen', node_size=700, font_size=9)
pl.title("Environment with Police and Drugs")
pl.show()